---
title:  "비정형데이터 - HTTP"
excerpt: "빅데이터 수집 / 전처리 : 비정형데이터 - HTTP"
toc: true
toc_sticky: true

categories:
  - Data Collection
tags:
  - [Data Collection, HTTP, Legal Issue, URL Handling Modules, RobotParser, Request]
last_modified_at: 2020-07-16 13:34:08
---


## 1. HTTP
- Hyper Text Transfer Protocol
- HTTP로 가져오면 기본적으로 document가 HTML이다.
- Request를 보내면 반드시 Response를 받아야 한다.

### Request
- Header와 Body(Optional)로 이루어져 있다.
  - **Header**: Get, Host, Accept, Accept-Language, Accept-Charset, User-agent, Connection 등으로 구성됨
- Header 예시

  ```http
  Get               /index.html   HTTP/1.1
  Host              www.example.com
  Accept            text/html, */*
  Accept-Language   en-us
  Accept-Charset    ISO-8859-1,utf-8
  User-agent        Mozilla/5.0
  Connection        Keep-alive
  ```

### HTTP Method
- GET
- HEAD
- PUT
- POST
- PATCH
- TRACE
- OPTIONS
- DELETE

### REST
- REST : Representational State Transfer
- CRUD : Create, Read, Update, Delete
- Restful

### Response
- status가 200이면 정삭적으로 되었다는 뜻

## 2. 법적 문제
### Opt-in vs. Opt-out
- **Opt-in**: 정보수집을 명시적으로 동의할 때에만 정보수집 가능
- **Opt-out** : 정보수집을 명시적으로 거부할 때에만 정보수집 중단
  - 웹에서는 이 방식을 사용하므로 Opt-out으로 명시된 사항들이 뭐가 있는지를 확인해야 함

### 합법
- 검색엔진(bot, spider, crawler etc.), 가격비교 등은 합법 
  - Opt-out
  - 공공의 목적이 있기 때문에

### 불법
- 봇이 개인정보, 지적재산권이 포함된 DB에서 정보를 가져오게 되면 불법  
- 과도한 트래픽을 유발하면 다른 이용자들이 이용하기 힘들어지기 때문에 불법
  - 텀을 길게 주는 등 과도한 트래픽을 발생시키지 않는 범위 내에서 크롤링되어야 한다.

### robots.txt
- Crawler와 같은 bot 접근을 제어하기 위한 규약
- 대상 봇, 수집 여부, 수집 범위 등을 기술
- 권고안
- 사이트이름/robots.txt로 찾을 수 있다.
  
모든 User-agent에 대해 허용
```
'''
User-agent:*
Allow:/
'''
```  

모든 User-agent에 대해 불허
```
'''
User-agent:*
Disallow:/
'''
```

|Name|User-Agent|
|----|----------|
|Google|Googlebot|
|Google image|Googlebot-image|
|Msn|MSNBot|
|Naver|Yeti|
|Daum|Daumoa|


### 유의하면 좋을 것들
- Robots.txt : 접근 제약 규칙 준수
- Crawl delay : 사이트에 최대한 부담 지양
- Term of use : 사이트 이용방침(약관) 준수
- Public Content : 지저재산권 침해 여부
- Authentication-based site : 민감한 정보 수집 주의


## 3. URL Handling Modules
- **urllib.request** : Opening and reading URLs
- **urllib.error** : Contaning the exceptions raised by urllib.request
- **urllib.parse** : Parsing URLs
- **urllib.robotparser** : Parsing robots.txt files
- **urllib.response** : Used internally by the urllib request module

### RobotParser

```py
from urllib import robotparser

robot = robotparser.RobotFileParser()
robot.set_url('https://news.naver.com/robots.txt')
robot.read()
robot.can_fetch('Yeti', '/main/imagemontage')
# True: Yeti라는 user-agent는 접근가능
```

### Request

```py
from urllib.request import urlopen, Request

resp = urlopen('http://www.google.com/robots.txt')

type(resp) # http.client.HTTPResponse

resp.getheaders() # ...

resp.code, resp.reason # (200, 'OK')

resp.info() # <http.client.HTTPMessage at 0x196bb36ce08>
```

read는 한 번 실행하면 사라지기 때문에 일단 담아두는 것이 좋다.

```py
body = resp.read()

# 정리해서 보기
[_.split(':') for _ in body.decode('UTF-8').split('\n')]
```
구글의 경우, Request 객체가 필요하다.
```py
req = Request(url='https://www.google.com/search?q=%EC%9C%A0%ED%8A%9C%EB%B8%8C&oq=%EC%9C%A0%ED%8A%9C%EB%B8%8C&aqs=chrome..69i57j69i59l3j0j69i61l3.1961j0j7&sourceid=chrome&ie=UTF-8')

# 아직 header는 없는 상태.
req.headers() # {}
```
header를 추가해보자. 필요한 user-agent를 찾아오기 위해서는 Chrome 개발자도구 -> Network -> 새로고침 -> 제일 위의 `search?q=...` 선택 -> Headers -> Request Headers -> user-agent 확인의 단계를 거치면 된다.  

```py
req.add_header('user-agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36')
```
```py
resp = urlopen(req)

resp.getheadersd()

resp.read().decode('utf-8')
```

Request 객체를 사용할 필요가 없는 경우, 다음과 같이 할 수 있다.

```py
resp = urlopen('https://search.naver.com/search.naver?sm=top_hty&fbm=1&ie=utf8&query=%EC%9B%A8%EC%9D%BC')

resp.code

resp.read().decode('utf8')
```

