---
title:  "머신러닝 앙상블 학습(Ensemble Learning) - 부스팅(Boosting) (2) : XGBoost"
excerpt: "각광받고 있는 트리 기반 앙상블 학습 알고리즘 중 하나인 XGBoost(eXtra Gradient Boost)에 대해 정리한 글입니다."
toc: true
toc_sticky: true

categories:
  - Data Analysis
tags:
  - [Data Analysis, Machine Learning, Classification, Ensemble, Boosting, Scikit learn, XGBoost]
last_modified_at: 2020-10-12 22:08:41
---

## 1. XGBoost (eXtra Gradient Boost)  

GBM 기반의 XGBoost는 다른 머신러닝보다 뛰어난 예측 성능을 보인다. 그러면서 GBM의 단점이었던 느린 수행 시간과 과적합 규제(Regularization) 부재의 문제 또한 극복했다는 점, Tree Pruning이 가능하다는 점, 자체적으로 내장된 교차 검증과 조기중단(Early Stopping), 결손값 처리 기능을 가지고 있다는 점도 XGBoost의 장점이다.   

> XGBoost도 랜덤 포레스트에 비해서는 느리다.   

## 2. XGBoost 주요 파라미터  

사이킷런 Wrapper에서 사용하는 XGBoost 파라미터들은 기본적으로 GBM의 파라미터와 유사하며, GBM에 해당하는 파라미터가 없다면 파이썬 Wrapper의 하이퍼 파라미터를 가져온다.   

> [GBM 하이퍼 파라미터 설명글 보기](https://ek-koh.github.io/data%20analysis/gbm/#3-gbm-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D)  

이전 글들에서 다루지 않았던 부분만 정리해보자. 사실 XGBoost에서 하이퍼 파라미터 튜닝에는 그렇게 공을 들이지 않는 편이 좋다.    

- `min_child_weight` : 결정트리의 `min_child_leaf`와 유사하며, 트리에서 추가적으로 가지를 나눌지를 결정하기 위해 필요한 데이터들의 weight 총합. 값이 클수록 분할을 자제하여 과적합을 조절하는 용도로 사용됨 (디폴트는 1)
- `gamma` : 트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값으로 값이 클수록 과적합 감소 효과가 있음 (디폴트는 0)
- `lambda` : L2 Regularizaiton 적용 값. 값이 클수록 과적합 감소 효과가 있음 (디폴트는 1)
- `alpha` : L1 Reuglarization 적용 값. 값이 클수록 과적합 감소 효과가 있음 (디폴트는 0)
- `colsample_bytree` : `max_features`와 유사하며, 트리 생성에 필요한 피처를 임의로 샘플링하는 데 사용됨 (디폴트는 1)
- `scale_pos_weight` : 비대칭 클래스로 구성된 데이터셋의 균형을 유지하기 위한 파라미터 (디폴트는 1)






