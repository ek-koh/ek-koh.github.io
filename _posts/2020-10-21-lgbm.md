---
title:  "머신러닝 앙상블 학습(Ensemble Learning) - 부스팅(Boosting) (3) : LightGBM"
excerpt: "각광받고 있는 트리 기반 앙상블 학습 알고리즘 중 하나인 LightGBM에 대해 정리한 글입니다."
toc: true
toc_sticky: true

categories:
  - Data Analysis
tags:
  - [Data Analysis, Machine Learning, Classification, Ensemble, Boosting, Scikit learn, LightGBM]
last_modified_at: 2020-10-21 23:32:37
---

## 1. LightGBM

LightGBM은 XGBoost를 보완하여 나온 알고리즘으로, XGBoost보다 속도가 더 빠르고 메모리 사용량도 더 적다. 그러면서도 예측 성능은 XGBoost와 거의 같다.  

LightGBM은 일반적인 GBM 계열이 **균형 트리 분할(Level Wise)** 방식을 사용했던 것과 다르게 **리프 중심 트리 분할(Leaf Wise)** 방식을 사용한다. LightGBM에 따르면, 이처럼 트리의 균형을 맞추지 않고 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성한 규칙 트리는 학습을 반복할수록 균형 트리 분할 방식보다 예측 오류 손실을 최소화할 수 있고 시간도 적게 걸리게 된다.  

![image](https://user-images.githubusercontent.com/58713684/96736100-390dfd00-13f7-11eb-88bb-6fe74735b185.png)
  

뿐만 아니라 LightGBM은 카테고리형 피처 자동 변환 및 최적 분할을 지원하기도 한다.  

## 2. LightGBM 주요 파라미터 (사이킷런 Wrapper)  

사이킷런 Wrapper에서 사용하는 LightGBM 파라미터들은 기본적으로 XGBoost의 파라미터와 유사하며, XGBoost에 해당하는 파라미터가 없다면 파이썬 Wrapper의 하이퍼 파라미터를 가져온다.   

> [XGBoost 하이퍼 파라미터 설명글 보기](https://ek-koh.github.io/data%20analysis/xgboost/#2-xgboost-%EC%A3%BC%EC%9A%94-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0-wrapper)  

이전 글들에서 다루지 않았던 부분이나 차이가 나는 부분만 정리해보자.  

- `max_depth` : 결정 트리의 `max_depth`와 동일하나, 리프 중심 트리 분할 방식을 사용하는만큼 `max_depth`를 매우 크게 가짐 (디폴트는 -1 : 0보다 작은 값이면 깊이 제한 없음)
- `min_child_samples` : 결정트리의 `min_samples_leaf`와 같은 파라미터로, 최종 결정 클래스인 리프 노드가 되기 위해 최소한으로 필요한 레코드 수 (디폴트는 20)

