---
title:  "머신러닝 앙상블 학습(Ensemble Learning) - 부스팅(Boosting) (1) : GBM"
excerpt: "앙상블 학습 방식 중 하나인 부스팅(Boosting) 알고리즘의 개념과 GBM에 대해 정리한 글입니다."
toc: true
toc_sticky: true

categories:
  - Data Analysis
tags:
  - [Data Analysis, Machine Learning, Classification, Ensemble, Boosting, GBM, Scikit learn]
last_modified_at: 2020-10-06 21:50:58
---

부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습하고 예측하면서 잘못 예측한 데이터에 가중치를 부여하며 오류를 개선해 나가는 방식이다. 대표적으로 AdaBoost(Adaptive Boosting), GBM(Gradient Boosting Machine), XGBoost(eXtra Gradient Boost), LightGBM(Light Gradient Boost)이 있다. 이 글에서는 GBM에 대해 다루고자 한다.  

## 1. GBM (Gradient Boosting Machine)  

GBM은 경사 하강법(Gradient Descent)를 이용해 가중치 업데이트를 수행하는 기법이다. GBM이 일반적으로 랜덤포레스트보다 성능이 뛰어나기는 하지만, 수행 시간이 오래 걸린다는 단점이 있다.    

> 경사하강법이란?
> 
> h(x) = y - F(x)라고 할 때, h(x)를 최소화하는 방향으로 가중치 값을 업데이트하는 방법
> 
> - h(x) : 오류식
> - y : 분류의 실제 결과값
> - F(x) : 피처 기반 예측함수(예측값)


## 2. 사이킷런에서 구현하기  

사이킷런은 GBM 기반의 분류를 위해 GradientBoostingClassifier 클래스를 제공한다.  

```py
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=0)

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train, y_train)
pred = gb_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred)
print('GBM 정확도: {0:.4f}'.format(accuracy)) # 0.9649
```  

## 3. GBM 하이퍼 파라미터 튜닝  

GBM에서 사용되는 하이퍼 파라미터는 결정트리, 랜덤 포레스트에서 사용되는 파라미터와 어느 정도 유사하다.  

> [결정트리 하이퍼 파라미터 설명글 보기](https://ek-koh.github.io/data%20analysis/decision-tree/#3-%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC%EC%9D%98-%EA%B3%BC%EC%A0%81%ED%95%A9)  

> [랜덤포레스트 하이퍼 파라미터 설명글 보기](https://ek-koh.github.io/data%20analysis/random-forest/#3-%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D)  

이 글에서는 추가적인 부분만 따로 정리했다.  

- `loss` : 경사 하강법에서 사용할 비용함수 지정 (디폴트는 deviance)
- `learning_rate` : 약한 학습기가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수로 0 ~ 1 값 지정 가능 (디폴트는 0.1)
  - 너무 작은 값 지정 -> 예측 성능은 높아질 수 있지만 수행 시간이 오래 걸릴 수 있고, 반복이 완료되어도 최소오류값을 찾지 못할 수 있음
  - 너무 큰 값 지정 -> 빠른 수행은 가능하지만 최소오류값을 못찾고 지나쳐 예측 성능이 저하될 수 있음
- `n_estimators` : 약한 학습기의 개수 (디폴트는 100)
  - 개수가 많을수록 예측 성능은 높아질 수 있지만 수행 시간이 오래 걸림
- `subsample` : 학습에 사용하는 데이터의 샘플링 비율로 0 ~ 1 값 지정 가능 (디폴트는 1, 즉 전체 학습 데이터 기반)  

GridSearchCV를 이용해 하이퍼 파라미터 튜닝을 해보자.  

```py
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators' : [100, 500],
    'learning_rate' : [0.05, 0.1]
}
grid_cv = GridSearchCV(gb_clf, param_grid=params, cv=2, verbose=1)
grid_cv.fit(X_train, y_train)
print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```  

```
최적 하이퍼 파라미터:
 {'learning_rate': 0.1, 'n_estimators': 500}
최고 예측 정확도: 0.9495
```  

튜닝된 하이퍼 파라미터로 재학습한 후 예측과 평가를 수행해보자.  

```py
pred = grid_cv.best_estimator_.predict(X_test)
accuracy = accuracy_score(y_test, pred)
print('GBM 정확도: {0:.4f}'.format(accuracy)) # 0.9649
```   



